#!/usr/bin/env python3
"""
æ™ºèƒ½å¯åŠ¨å™¨ - AI å­—å¹•ç”Ÿæˆå™¨
ğŸš€ è‡ªåŠ¨æ£€æµ‹å¹¶ä¼˜åŒ– NVIDIA / AMD / Apple Silicon / CPU ç¯å¢ƒ
"""
import os
import sys
import subprocess
import logging
import socket

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

def print_header():
    """æ‰“å°å¯åŠ¨å¤´éƒ¨"""
    print("ğŸš€ AI å­—å¹•ç”Ÿæˆå™¨ - æ™ºèƒ½å¯åŠ¨å™¨")
    print("=" * 60)
    print("ğŸ¯ è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¡ç®—è®¾å¤‡å¹¶ä¼˜åŒ–æ€§èƒ½é…ç½®")
    print("ğŸ’ª æ”¯æŒ: NVIDIA CUDA / AMD ROCm / Apple MPS / CPU")
    print("=" * 60)

def check_dependencies():
    """æ£€æŸ¥åŸºç¡€ä¾èµ–"""
    try:
        import torch
        import whisper
        import flask
        logger.info("âœ… æ ¸å¿ƒä¾èµ–æ£€æŸ¥é€šè¿‡")
        return True
    except ImportError as e:
        logger.error(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        logger.error("ğŸ’¡ è¯·è¿è¡Œ: pip install -r requirements.txt")
        return False

def setup_environment():
    """è®¾ç½®ç¯å¢ƒå’Œå¯¼å…¥GPUæ£€æµ‹"""
    try:
        from gpu_detector import GPUDetector, create_device_environment
        
        # åˆ›å»ºæ£€æµ‹å™¨
        detector = GPUDetector()
        
        # æ‰“å°æ£€æµ‹æŠ¥å‘Š
        print("\nğŸ” GPU ç¯å¢ƒæ£€æµ‹:")
        print("-" * 40)
        print(f"ğŸ“Š {detector.get_device_summary()}")
        
        # åº”ç”¨ç¯å¢ƒå˜é‡ä¼˜åŒ–
        device_env = create_device_environment()
        for key, value in device_env.items():
            os.environ[key] = value
            
        # æ˜¾ç¤ºå…³é”®ä¼˜åŒ–è®¾ç½®
        device_type = detector.device_info['gpu_type']
        if device_type == 'nvidia':
            print("ğŸŸ¢ NVIDIA CUDA ä¼˜åŒ–å·²å¯ç”¨")
        elif device_type == 'amd':
            print("ğŸ”´ AMD ROCm ä¼˜åŒ–å·²å¯ç”¨")
        elif device_type == 'apple':
            print("ğŸŸ¡ Apple MPS ä¼˜åŒ–å·²å¯ç”¨")
        else:
            print("ğŸ”µ CPU å¤šçº¿ç¨‹ä¼˜åŒ–å·²å¯ç”¨")
            
        # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
        tips = detector.get_optimization_tips()
        if tips:
            print(f"ğŸ’¡ {tips[0]}")  # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå»ºè®®
            
        return detector
        
    except ImportError:
        logger.warning("âš ï¸ GPU æ£€æµ‹æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return None

def check_model_availability():
    """æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§"""
    print("\nğŸ“¦ æ£€æŸ¥æ¨¡å‹çŠ¶æ€:")
    print("-" * 40)
    
    try:
        import whisper
        
        # æ£€æŸ¥å¸¸ç”¨æ¨¡å‹
        models_to_check = ['base', 'small', 'medium']
        available_models = []
        
        for model in models_to_check:
            try:
                # å¿«é€Ÿæ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
                model_path = whisper._MODELS[model]
                if os.path.exists(os.path.expanduser(f"~/.cache/whisper/{model_path.split('/')[-1]}")):
                    available_models.append(model)
                    print(f"   âœ… {model}")
                else:
                    print(f"   â³ {model} (é¦–æ¬¡ä½¿ç”¨å°†è‡ªåŠ¨ä¸‹è½½)")
            except:
                print(f"   â“ {model} (çŠ¶æ€æœªçŸ¥)")
        
        if available_models:
            print(f"ğŸ“Š å·²ç¼“å­˜ {len(available_models)} ä¸ªæ¨¡å‹")
        else:
            print("ğŸ’¡ é¦–æ¬¡è¿è¡Œå°†è‡ªåŠ¨ä¸‹è½½æ‰€éœ€æ¨¡å‹")
            
    except Exception as e:
        logger.warning(f"âš ï¸ æ¨¡å‹æ£€æŸ¥å¤±è´¥: {e}")

def get_performance_recommendations(detector):
    """è·å–æ€§èƒ½å»ºè®®"""
    if not detector:
        return []
        
    recommendations = []
    device_info = detector.device_info
    
    # åŸºäºè®¾å¤‡ç±»å‹ç»™å‡ºå»ºè®®
    if device_info['gpu_type'] == 'nvidia':
        if device_info['memory_gb'] >= 12:
            recommendations.append("ğŸ’ª æ˜¾å­˜å……è¶³ï¼Œå¯ä½¿ç”¨ large æ¨¡å‹è·å¾—æœ€ä½³æ•ˆæœ")
        elif device_info['memory_gb'] >= 8:
            recommendations.append("ğŸ‘ æ¨èä½¿ç”¨ medium æˆ– small æ¨¡å‹å¹³è¡¡æ€§èƒ½")
        else:
            recommendations.append("âš¡ æ˜¾å­˜æœ‰é™ï¼Œå»ºè®®ä½¿ç”¨ base æˆ– small æ¨¡å‹")
            
    elif device_info['gpu_type'] == 'amd':
        recommendations.append("ğŸ® AMD GPU åŠ é€Ÿï¼Œæ¨èä½¿ç”¨ small æˆ– base æ¨¡å‹")
        recommendations.append("ğŸ”§ å¦‚é‡é—®é¢˜å¯å›é€€ CPU æ¨¡å¼")
        
    elif device_info['gpu_type'] == 'apple':
        recommendations.append("ğŸ Apple Silicon ä¼˜åŒ–ï¼Œmedium æ¨¡å‹æ€§èƒ½è‰¯å¥½")
        
    else:
        recommendations.append("âš¡ CPU æ¨¡å¼ï¼Œæ¨è base æ¨¡å‹è·å¾—æœ€ä½³é€Ÿåº¦")
        
    return recommendations

def show_startup_info(detector):
    """æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯"""
    print(f"\nğŸŒ æœåŠ¡åœ°å€:")
    print("-" * 40)
    print(f"   ğŸ“º å®æ—¶è½¬å½•: http://localhost:5001/realtime.html")
    print(f"   ğŸ¬ æ–‡ä»¶å¤„ç†: http://localhost:5001/app.html")
    
    # æ˜¾ç¤ºæ€§èƒ½å»ºè®®
    recommendations = get_performance_recommendations(detector)
    if recommendations:
        print(f"\nğŸ¯ æ€§èƒ½å»ºè®®:")
        print("-" * 40)
        for rec in recommendations:
            print(f"   {rec}")
    
    print(f"\nğŸ”” ä½¿ç”¨æç¤º:")
    print("-" * 40)
    print(f"   â€¢ é¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…")
    print(f"   â€¢ å¤§æ–‡ä»¶å¤„ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
    print(f"   â€¢ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print(f"   â€¢ é‡åˆ°é—®é¢˜å¯æŸ¥çœ‹ç»ˆç«¯è¾“å‡ºä¿¡æ¯")

def check_port_availability():
    """æ£€æŸ¥ç«¯å£å¯ç”¨æ€§"""
    def is_port_in_use(port):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            return s.connect_ex(('localhost', port)) == 0
    
    if is_port_in_use(5001):
        print("âš ï¸ ç«¯å£ 5001 å·²è¢«å ç”¨")
        print("ğŸ’¡ å¯èƒ½å·²æœ‰å®ä¾‹åœ¨è¿è¡Œï¼Œæˆ–å…¶ä»–ç¨‹åºå ç”¨äº†è¯¥ç«¯å£")
        
        response = input("æ˜¯å¦ç»§ç»­å¯åŠ¨ï¼Ÿ(y/N): ").lower().strip()
        if response != 'y':
            print("ğŸ‘‹ å¯åŠ¨å·²å–æ¶ˆ")
            return False
    
    return True

def start_application():
    """å¯åŠ¨åº”ç”¨"""
    print(f"\nğŸš€ æ­£åœ¨å¯åŠ¨ AI å­—å¹•ç”Ÿæˆå™¨...")
    print("=" * 60)
    
    try:
        # å¯åŠ¨ Flask åº”ç”¨
        subprocess.run([sys.executable, "app.py"], check=True)
        
    except KeyboardInterrupt:
        print(f"\n\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")
        print("æ„Ÿè°¢ä½¿ç”¨ AI å­—å¹•ç”Ÿæˆå™¨ï¼")
        
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ å¯åŠ¨å¤±è´¥: é€€å‡ºç  {e.returncode}")
        show_troubleshooting()
        
    except Exception as e:
        print(f"\nâŒ æ„å¤–é”™è¯¯: {e}")
        show_troubleshooting()

def show_troubleshooting():
    """æ˜¾ç¤ºæ•…éšœæ’é™¤ä¿¡æ¯"""
    print(f"\nğŸ”§ æ•…éšœæ’é™¤:")
    print("-" * 40)
    print(f"   1. æ£€æŸ¥ä¾èµ–: pip install -r requirements.txt")
    print(f"   2. æ£€æŸ¥ Python ç‰ˆæœ¬: python --version (éœ€è¦ 3.8+)")
    print(f"   3. æ£€æŸ¥ç«¯å£å ç”¨: lsof -i :5001")
    print(f"   4. æŸ¥çœ‹è¯¦ç»†æ—¥å¿—: python app.py")
    print(f"   5. é‡ç½®ç¯å¢ƒ: åˆ é™¤è™šæ‹Ÿç¯å¢ƒé‡æ–°å®‰è£…")
    
    print(f"\nğŸ†˜ GPU ç›¸å…³é—®é¢˜:")
    print(f"   â€¢ NVIDIA: æ£€æŸ¥ CUDA é©±åŠ¨å’Œ PyTorch CUDA ç‰ˆæœ¬")
    print(f"   â€¢ AMD: ç¡®ä¿å®‰è£… ROCm å’Œ PyTorch ROCm ç‰ˆæœ¬") 
    print(f"   â€¢ é€šç”¨: å¯è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶ CPU æ¨¡å¼")
    print(f"     export CUDA_VISIBLE_DEVICES=-1")

def main():
    """ä¸»å‡½æ•°"""
    print_header()
    
    # åŸºç¡€æ£€æŸ¥
    if not check_dependencies():
        return 1
    
    # ç«¯å£æ£€æŸ¥
    if not check_port_availability():
        return 1
        
    # ç¯å¢ƒè®¾ç½®å’ŒGPUæ£€æµ‹
    detector = setup_environment()
    
    # æ¨¡å‹æ£€æŸ¥
    check_model_availability()
    
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    show_startup_info(detector)
    
    # å¯åŠ¨åº”ç”¨
    start_application()
    
    return 0

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except Exception as e:
        logger.error(f"ğŸ’¥ å¯åŠ¨å™¨å¼‚å¸¸: {e}")
        sys.exit(1)