#!/usr/bin/env python3
"""
GPUæ£€æµ‹æµ‹è¯•è„šæœ¬
ğŸ§ª æµ‹è¯•é€šç”¨GPUæ£€æµ‹å’Œé€‚é…ç³»ç»Ÿ
"""
import sys
import os

def test_gpu_detector():
    """æµ‹è¯•GPUæ£€æµ‹å™¨åŠŸèƒ½"""
    print("ğŸ§ª GPU æ£€æµ‹å™¨æµ‹è¯•")
    print("=" * 50)
    
    try:
        from gpu_detector import GPUDetector, get_optimal_device, create_device_environment
        
        # åˆ›å»ºæ£€æµ‹å™¨
        detector = GPUDetector()
        
        # æ‰“å°å®Œæ•´æ£€æµ‹æŠ¥å‘Š
        detector.print_detection_report()
        
        # æµ‹è¯•æœ€ä½³è®¾å¤‡é€‰æ‹©
        print(f"\nğŸ¯ è®¾å¤‡é€‰æ‹©æµ‹è¯•:")
        print("-" * 30)
        device, device_info = get_optimal_device()
        print(f"æ¨èè®¾å¤‡: {device}")
        print(f"è®¾å¤‡ç±»å‹: {device_info['gpu_type']}")
        print(f"æ€§èƒ½ç­‰çº§: {device_info['performance_level']}")
        
        # æµ‹è¯•ç¯å¢ƒå˜é‡
        print(f"\nğŸ”§ ç¯å¢ƒå˜é‡æµ‹è¯•:")
        print("-" * 30)
        env_vars = create_device_environment()
        for key, value in list(env_vars.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   {key}={value}")
        if len(env_vars) > 5:
            print(f"   ... å…± {len(env_vars)} ä¸ªç¯å¢ƒå˜é‡")
        
        print(f"\nâœ… GPUæ£€æµ‹å™¨æµ‹è¯•å®Œæˆ")
        return True
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_pytorch_integration():
    """æµ‹è¯•PyTorché›†æˆ"""
    print(f"\nğŸ”¥ PyTorch é›†æˆæµ‹è¯•")
    print("=" * 50)
    
    try:
        import torch
        print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
        
        # æµ‹è¯•è®¾å¤‡å¯ç”¨æ€§
        devices_tested = []
        
        # CUDAæµ‹è¯•
        if torch.cuda.is_available():
            print(f"âœ… CUDA å¯ç”¨")
            print(f"   è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
            devices_tested.append('cuda')
            
            # ç®€å•è®¡ç®—æµ‹è¯•
            try:
                x = torch.randn(1000, 1000).cuda()
                y = torch.matmul(x, x)
                print(f"   CUDA è®¡ç®—æµ‹è¯•: âœ… é€šè¿‡")
                del x, y
                torch.cuda.empty_cache()
            except Exception as e:
                print(f"   CUDA è®¡ç®—æµ‹è¯•: âŒ å¤±è´¥ - {e}")
        else:
            print(f"âŒ CUDA ä¸å¯ç”¨")
        
        # MPSæµ‹è¯• (Apple Silicon)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print(f"âœ… MPS (Apple Silicon) å¯ç”¨")
            devices_tested.append('mps')
            
            try:
                x = torch.randn(1000, 1000).to('mps')
                y = torch.matmul(x, x)
                print(f"   MPS è®¡ç®—æµ‹è¯•: âœ… é€šè¿‡")
                del x, y
            except Exception as e:
                print(f"   MPS è®¡ç®—æµ‹è¯•: âŒ å¤±è´¥ - {e}")
        else:
            print(f"âŒ MPS ä¸å¯ç”¨")
        
        # CPUæµ‹è¯•
        print(f"âœ… CPU å¯ç”¨")
        devices_tested.append('cpu')
        try:
            x = torch.randn(1000, 1000)
            y = torch.matmul(x, x)
            print(f"   CPU è®¡ç®—æµ‹è¯•: âœ… é€šè¿‡")
            del x, y
        except Exception as e:
            print(f"   CPU è®¡ç®—æµ‹è¯•: âŒ å¤±è´¥ - {e}")
        
        print(f"\nğŸ“Š å¯ç”¨è®¾å¤‡: {', '.join(devices_tested)}")
        return True
        
    except ImportError:
        print(f"âŒ PyTorch æœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âŒ PyTorch æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_whisper_compatibility():
    """æµ‹è¯•Whisperå…¼å®¹æ€§"""
    print(f"\nğŸ¤ Whisper å…¼å®¹æ€§æµ‹è¯•")
    print("=" * 50)
    
    try:
        import whisper
        print(f"âœ… Whisper å·²å®‰è£…")
        
        # è·å–å¯ç”¨æ¨¡å‹
        models = list(whisper._MODELS.keys())
        print(f"ğŸ“¦ å¯ç”¨æ¨¡å‹: {', '.join(models)}")
        
        # æ£€æŸ¥ç¼“å­˜çš„æ¨¡å‹
        cache_dir = os.path.expanduser("~/.cache/whisper")
        if os.path.exists(cache_dir):
            cached_files = os.listdir(cache_dir)
            cached_models = [f for f in cached_files if f.endswith('.pt')]
            if cached_models:
                print(f"ğŸ’¾ å·²ç¼“å­˜æ¨¡å‹: {len(cached_models)} ä¸ª")
            else:
                print(f"ğŸ’¾ æš‚æ— ç¼“å­˜æ¨¡å‹")
        else:
            print(f"ğŸ’¾ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        
        return True
        
    except ImportError:
        print(f"âŒ Whisper æœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âŒ Whisper æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_environment_setup():
    """æµ‹è¯•ç¯å¢ƒè®¾ç½®"""
    print(f"\nğŸŒ ç¯å¢ƒè®¾ç½®æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥å½“å‰ç¯å¢ƒå˜é‡
    gpu_related_vars = [
        'CUDA_VISIBLE_DEVICES', 'ROCM_PATH', 'HSA_OVERRIDE_GFX_VERSION',
        'PYTORCH_CUDA_ALLOC_CONF', 'PYTORCH_HIP_ALLOC_CONF',
        'OMP_NUM_THREADS', 'MKL_NUM_THREADS'
    ]
    
    print("ğŸ” å½“å‰GPUç›¸å…³ç¯å¢ƒå˜é‡:")
    for var in gpu_related_vars:
        value = os.environ.get(var)
        if value:
            print(f"   {var}={value}")
        else:
            print(f"   {var}=æœªè®¾ç½®")
    
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª AI å­—å¹•ç”Ÿæˆå™¨ - GPU æ£€æµ‹ç³»ç»Ÿæµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("GPUæ£€æµ‹å™¨", test_gpu_detector),
        ("PyTorché›†æˆ", test_pytorch_integration), 
        ("Whisperå…¼å®¹æ€§", test_whisper_compatibility),
        ("ç¯å¢ƒè®¾ç½®", test_environment_setup)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
    print(f"\nğŸ æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 60)
    
    passed = 0
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ“Š é€šè¿‡ç‡: {passed}/{len(results)} ({passed/len(results)*100:.1f}%)")
    
    if passed == len(results):
        print(f"ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå‡†å¤‡å°±ç»ª")
        return 0
    else:
        print(f"âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
        return 1

if __name__ == "__main__":
    sys.exit(main())