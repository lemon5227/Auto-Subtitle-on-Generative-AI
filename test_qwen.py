#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwenæ™ºèƒ½å­—å¹•æ ¡å¯¹åŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•Qwenæ¨¡å‹çš„åŠ è½½ã€æ¨ç†å’Œå­—å¹•ä¼˜åŒ–åŠŸèƒ½
æ”¯æŒé¡¹ç›®å¯åŠ¨å‰é¢„ä¸‹è½½æ¨¡å‹
"""

import sys
import time
import argparse
import os

def test_qwen_import():
    """æµ‹è¯•Qwenç›¸å…³åº“å¯¼å…¥"""
    print("=" * 60)
    print("1. æµ‹è¯•Qwenç›¸å…³åº“å¯¼å…¥")
    print("=" * 60)
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        print("âœ… transformersåº“å¯¼å…¥æˆåŠŸ")
        
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   GPUå‹å·: {torch.cuda.get_device_name(0)}")
            print(f"   GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        
        return True
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_gpu_detection():
    """æµ‹è¯•GPUæ£€æµ‹åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("2. æµ‹è¯•GPUæ£€æµ‹åŠŸèƒ½")
    print("=" * 60)
    
    try:
        from gpu_detector import GPUDetector
        
        detector = GPUDetector()
        device = detector.get_optimal_device()
        info = detector.get_device_info()
        
        print(f"âœ… æ£€æµ‹åˆ°è®¾å¤‡: {device}")
        print(f"   è®¾å¤‡ç±»å‹: {info['type']}")
        print(f"   è®¾å¤‡åç§°: {info['name']}")
        print(f"   æ€§èƒ½è¯„çº§: {info['performance']}")
        
        if info['memory']:
            print(f"   æ˜¾å­˜ä¿¡æ¯: {info['memory']}")
        
        print(f"\nğŸ’¡ æ¨è: {info['recommendation']}")
        
        return True, device
    except Exception as e:
        print(f"âŒ GPUæ£€æµ‹å¤±è´¥: {e}")
        return False, 'cpu'

def test_qwen_model_info():
    """æµ‹è¯•Qwenæ¨¡å‹ä¿¡æ¯è·å–"""
    print("\n" + "=" * 60)
    print("3. æµ‹è¯•Qwenæ¨¡å‹ä¿¡æ¯")
    print("=" * 60)
    
    try:
        # ä»…Qwen3ç³»åˆ—æ¨¡å‹ï¼ˆç§»é™¤è¿‡æœŸçš„Qwen2.5ï¼‰
        models = [
            {"name": "Qwen3-0.6B", "model_id": "Qwen/Qwen3-0.6B", "size": "0.6B", "best_for": "å®æ—¶ç¿»è¯‘ï¼ˆè¶…è½»é‡ï¼‰", "vram": "~2GB"},
            {"name": "Qwen3-1.7B", "model_id": "Qwen/Qwen3-1.7B", "size": "1.7B", "best_for": "å®æ—¶ç¿»è¯‘ï¼ˆæ¨èï¼‰", "vram": "~4GB"},
            {"name": "Qwen3-4B", "model_id": "Qwen/Qwen3-4B", "size": "4B", "best_for": "å­—å¹•ä¼˜åŒ–ï¼ˆæ¨èï¼‰", "vram": "~8GB"},
            {"name": "Qwen3-8B", "model_id": "Qwen/Qwen3-8B", "size": "8B", "best_for": "å­—å¹•ä¼˜åŒ–ï¼ˆé«˜è´¨é‡ï¼‰", "vram": "~16GB"},
        ]
        
        print("âœ… æ”¯æŒçš„Qwen3æ¨¡å‹ï¼ˆå·²ç§»é™¤è¿‡æœŸçš„Qwen2.5ï¼‰:")
        for i, model in enumerate(models, 1):
            print(f"\n   {i}. {model['name']} ({model['size']})")
            print(f"      Model ID: {model['model_id']}")
            print(f"      é€‚ç”¨åœºæ™¯: {model['best_for']}")
            print(f"      æ˜¾å­˜éœ€æ±‚: {model['vram']}")
        
        return True, models
    except Exception as e:
        print(f"âŒ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        return False, []

def download_qwen_model(model_id, device='cpu', use_fp16=False):
    """ä¸‹è½½å¹¶æµ‹è¯•Qwenæ¨¡å‹
    
    Args:
        model_id: æ¨¡å‹ID (ä¾‹å¦‚: Qwen/Qwen3-1.7B)
        device: è®¾å¤‡ç±»å‹ (cpu/cuda)
        use_fp16: æ˜¯å¦ä½¿ç”¨FP16ç²¾åº¦
    """
    print("\n" + "=" * 60)
    print(f"ä¸‹è½½æ¨¡å‹: {model_id}")
    print("=" * 60)
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_id}")
        print(f"   è®¾å¤‡: {device}")
        print(f"   ç²¾åº¦: {'FP16' if use_fp16 else 'FP32'}")
        print(f"   ç¼“å­˜ç›®å½•: {os.path.expanduser('~/.cache/huggingface')}")
        
        start_time = time.time()
        
        # ä¸‹è½½tokenizer
        print("\n1ï¸âƒ£ ä¸‹è½½Tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            trust_remote_code=True
        )
        print("âœ… Tokenizerä¸‹è½½å®Œæˆ")
        
        # ä¸‹è½½æ¨¡å‹
        print("\n2ï¸âƒ£ ä¸‹è½½æ¨¡å‹æƒé‡...")
        dtype = torch.float16 if use_fp16 and device == 'cuda' else torch.float32
        
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=dtype,
            device_map="auto" if device == 'cuda' else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        if device == 'cpu':
            model = model.to('cpu')
        
        download_time = time.time() - start_time
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ (è€—æ—¶: {download_time:.1f}ç§’)")
        
        # æµ‹è¯•æ¨ç†
        print("\n3ï¸âƒ£ æµ‹è¯•æ¨¡å‹æ¨ç†...")
        test_text = "æµ‹è¯•æ–‡æœ¬ï¼šè¿™æ˜¯ä¸€ä¸ªç®€å•çš„æµ‹è¯•"
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"},
            {"role": "user", "content": test_text}
        ]
        
        text_input = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        model_inputs = tokenizer([text_input], return_tensors="pt")
        
        if device == 'cuda':
            model_inputs = model_inputs.to('cuda')
        
        with torch.no_grad():
            start_inference = time.time()
            generated_ids = model.generate(
                model_inputs.input_ids,
                max_new_tokens=50,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
            inference_time = time.time() - start_inference
        
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        print(f"âœ… æ¨ç†æµ‹è¯•æˆåŠŸ (è€—æ—¶: {inference_time:.2f}ç§’)")
        print(f"   è¾“å…¥: {test_text}")
        print(f"   è¾“å‡º: {response[:100]}...")
        
        # æ˜¾ç¤ºæ¨¡å‹å¤§å°
        if device == 'cuda':
            memory_allocated = torch.cuda.memory_allocated() / 1024**3
            print(f"\nğŸ“Š æ˜¾å­˜å ç”¨: {memory_allocated:.2f} GB")
        
        print(f"\nâœ… æ¨¡å‹ {model_id} ä¸‹è½½å¹¶æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def download_all_models(device='cpu', use_fp16=False, model_filter=None):
    """ä¸‹è½½æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹
    
    Args:
        device: è®¾å¤‡ç±»å‹
        use_fp16: æ˜¯å¦ä½¿ç”¨FP16
        model_filter: æ¨¡å‹è¿‡æ»¤å™¨ (realtime/refinement/all)
    """
    print("\n" + "=" * 60)
    print("æ‰¹é‡ä¸‹è½½Qwenæ¨¡å‹")
    print("=" * 60)
    
    models_to_download = []
    
    if model_filter == 'realtime':
        models_to_download = [
            "Qwen/Qwen3-0.6B",
            "Qwen/Qwen3-1.7B",
        ]
        print("ğŸ“¦ ä¸‹è½½å®æ—¶ç¿»è¯‘æ¨¡å‹ï¼ˆè½»é‡çº§ï¼‰")
    elif model_filter == 'refinement':
        models_to_download = [
            "Qwen/Qwen3-4B",
        ]
        print("ğŸ“¦ ä¸‹è½½å­—å¹•ä¼˜åŒ–æ¨¡å‹ï¼ˆæ ‡å‡†ï¼‰")
    else:  # all
        models_to_download = [
            "Qwen/Qwen3-0.6B",
            "Qwen/Qwen3-1.7B",
            "Qwen/Qwen3-4B",
            "Qwen/Qwen3-8B",
        ]
        print("ğŸ“¦ ä¸‹è½½æ‰€æœ‰Qwen3æ¨¡å‹")
    
    print(f"å…± {len(models_to_download)} ä¸ªæ¨¡å‹")
    
    success_count = 0
    for i, model_id in enumerate(models_to_download, 1):
        print(f"\n{'='*60}")
        print(f"è¿›åº¦: [{i}/{len(models_to_download)}]")
        print(f"{'='*60}")
        
        if download_qwen_model(model_id, device, use_fp16):
            success_count += 1
        else:
            print(f"âš ï¸ æ¨¡å‹ {model_id} ä¸‹è½½å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª...")
    
    print("\n" + "=" * 60)
    print(f"ä¸‹è½½å®Œæˆ: {success_count}/{len(models_to_download)} ä¸ªæ¨¡å‹æˆåŠŸ")
    print("=" * 60)
    
    return success_count == len(models_to_download)

def test_rule_based_refinement():
    """æµ‹è¯•åŸºäºè§„åˆ™çš„å­—å¹•ä¼˜åŒ–"""
    print("\n" + "=" * 60)
    print("4. æµ‹è¯•åŸºäºè§„åˆ™çš„å­—å¹•ä¼˜åŒ–")
    print("=" * 60)
    
    test_cases = [
        {
            "input": "å—¯è¿™ä¸ªå‘ƒæ–¹æ³•å¾ˆæœ‰æ•ˆ",
            "expected_improvements": ["å»é™¤å£è¯­è¯", "æ ‡ç‚¹ä¿®æ­£"]
        },
        {
            "input": "æˆ‘ä»¬ä»Šå¤©   è¦è®¨è®ºçš„æ˜¯    äººå·¥æ™ºèƒ½",
            "expected_improvements": ["ç©ºæ ¼ä¿®æ­£"]
        },
        {
            "input": "è¿™ä¸ªé¡¹ç›®å·²ç»å®Œæˆäº†",
            "expected_improvements": ["æ ‡ç‚¹è¡¥å……"]
        }
    ]
    
    import re
    success_count = 0
    
    for i, case in enumerate(test_cases, 1):
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i}:")
        print(f"   åŸæ–‡: {case['input']}")
        
        # æ¨¡æ‹Ÿè§„åˆ™ä¼˜åŒ–
        refined = case['input']
        
        # å»é™¤å£è¯­è¯
        fillers = ['å—¯', 'å•Š', 'å‘ƒ', 'é‚£ä¸ª', 'è¿™ä¸ª', 'å°±æ˜¯è¯´']
        for filler in fillers:
            refined = refined.replace(filler, '')
        
        # ä¿®æ­£ç©ºæ ¼
        refined = re.sub(r'\s+', ' ', refined).strip()
        
        # æ·»åŠ æ ‡ç‚¹
        if refined and not refined[-1] in '.!?ã€‚ï¼ï¼Ÿ':
            refined += 'ã€‚'
        
        print(f"   ä¼˜åŒ–: {refined}")
        print(f"   æ”¹è¿›: {', '.join(case['expected_improvements'])}")
        
        if refined != case['input']:
            print("   âœ… ä¼˜åŒ–æˆåŠŸ")
            success_count += 1
        else:
            print("   âš ï¸ æ— éœ€ä¼˜åŒ–")
    
    print(f"\næ€»ç»“: {success_count}/{len(test_cases)} æµ‹è¯•é€šè¿‡")
    return success_count == len(test_cases)

def test_qwen_model_loading(device='cpu'):
    """æµ‹è¯•Qwenæ¨¡å‹åŠ è½½ï¼ˆå¯é€‰ï¼Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼‰"""
    print("\n" + "=" * 60)
    print("5. æµ‹è¯•Qwenæ¨¡å‹åŠ è½½ï¼ˆå¯é€‰ï¼‰")
    print("=" * 60)
    print("âš ï¸ æ­¤æµ‹è¯•éœ€è¦ä¸‹è½½Qwenæ¨¡å‹ï¼ˆçº¦3-6GBï¼‰")
    print("å¦‚æœæ¨¡å‹æœªä¸‹è½½ï¼Œå°†è·³è¿‡æ­¤æµ‹è¯•")
    
    user_input = input("\næ˜¯å¦å°è¯•åŠ è½½Qwenæ¨¡å‹ï¼Ÿ(y/N): ").strip().lower()
    
    if user_input != 'y':
        print("â­ï¸ è·³è¿‡æ¨¡å‹åŠ è½½æµ‹è¯•")
        return True
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        
        model_id = "Qwen/Qwen2.5-1.5B-Instruct"  # ä½¿ç”¨æœ€å°çš„æ¨¡å‹
        print(f"\nğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_id}")
        print("   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼ˆé¦–æ¬¡éœ€è¦ä¸‹è½½ï¼‰...")
        
        start_time = time.time()
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            trust_remote_code=True
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if device != 'cpu' else torch.float32,
            device_map="auto" if device != 'cpu' else None,
            trust_remote_code=True
        )
        
        if device == 'cpu':
            model = model.to('cpu')
        
        load_time = time.time() - start_time
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (è€—æ—¶: {load_time:.2f}ç§’)")
        print(f"   è®¾å¤‡: {device}")
        print(f"   å‚æ•°é‡: çº¦1.5B")
        
        # æµ‹è¯•ç®€å•æ¨ç†
        print("\nğŸ§ª æµ‹è¯•æ¨ç†åŠŸèƒ½...")
        test_text = "æˆ‘ä»¬åœ¨åº§ä¸€ä¸ªæ–°çš„é¡¹ç›®"
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå­—å¹•æ ¡å¯¹åŠ©æ‰‹ï¼Œä¿®æ­£è¯†åˆ«é”™è¯¯ã€‚"},
            {"role": "user", "content": f"ä¿®æ­£è¿™å¥è¯: {test_text}"}
        ]
        
        text_input = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        model_inputs = tokenizer([text_input], return_tensors="pt")
        
        if device != 'cpu':
            model_inputs = model_inputs.to(device)
        
        with torch.no_grad():
            generated_ids = model.generate(
                model_inputs.input_ids,
                max_new_tokens=64,
                temperature=0.3,
                do_sample=True,
            )
        
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        print(f"   åŸæ–‡: {test_text}")
        print(f"   Qwenä¼˜åŒ–: {response}")
        print("âœ… æ¨ç†æµ‹è¯•æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½/æ¨ç†å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿ç½‘ç»œç•…é€šï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä»HuggingFaceä¸‹è½½")
        print("   å¯ä»¥è®¾ç½®é•œåƒåŠ é€Ÿ: export HF_ENDPOINT=https://hf-mirror.com")
        return False

def test_api_endpoints():
    """æµ‹è¯•APIç«¯ç‚¹ï¼ˆéœ€è¦å¯åŠ¨æœåŠ¡ï¼‰"""
    print("\n" + "=" * 60)
    print("6. æµ‹è¯•APIç«¯ç‚¹")
    print("=" * 60)
    print("âš ï¸ æ­¤æµ‹è¯•éœ€è¦å…ˆå¯åŠ¨FlaskæœåŠ¡: python app.py")
    
    user_input = input("\næœåŠ¡æ˜¯å¦å·²å¯åŠ¨ï¼Ÿ(y/N): ").strip().lower()
    
    if user_input != 'y':
        print("â­ï¸ è·³è¿‡APIæµ‹è¯•")
        print("ğŸ’¡ å¯åŠ¨æœåŠ¡åå¯ä»¥æ‰‹åŠ¨æµ‹è¯•:")
        print("   curl -X GET http://localhost:5001/api/qwen_models")
        print("   curl -X POST http://localhost:5001/api/refine_subtitle -H 'Content-Type: application/json' -d '{\"text\":\"æµ‹è¯•\",\"model\":\"local\"}'")
        return True
    
    try:
        import requests
        
        # æµ‹è¯•æ¨¡å‹åˆ—è¡¨API
        print("\næµ‹è¯• GET /api/qwen_models")
        response = requests.get('http://localhost:5001/api/qwen_models', timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… APIå“åº”æˆåŠŸ")
            print(f"   Qwenå¯ç”¨: {data.get('available')}")
            print(f"   æ”¯æŒæ¨¡å‹æ•°: {len(data.get('models', []))}")
        else:
            print(f"âŒ APIå“åº”å¤±è´¥: {response.status_code}")
            return False
        
        # æµ‹è¯•å­—å¹•ä¼˜åŒ–API
        print("\næµ‹è¯• POST /api/refine_subtitle")
        test_data = {
            "text": "å—¯è¿™ä¸ªå‘ƒæ–¹æ³•å¾ˆæœ‰æ•ˆ",
            "model": "local",
            "options": {
                "fix_punctuation": True,
                "remove_fillers": True
            }
        }
        
        response = requests.post(
            'http://localhost:5001/api/refine_subtitle',
            json=test_data,
            timeout=10
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… å­—å¹•ä¼˜åŒ–æˆåŠŸ")
            print(f"   åŸæ–‡: {data.get('original_text')}")
            print(f"   ä¼˜åŒ–: {data.get('refined_text')}")
            print(f"   æ¨¡å‹: {data.get('model')}")
        else:
            print(f"âŒ APIå“åº”å¤±è´¥: {response.status_code}")
            return False
        
        return True
        
    except ImportError:
        print("âŒ ç¼ºå°‘requestsåº“: pip install requests")
        return False
    except Exception as e:
        print(f"âŒ APIæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    parser = argparse.ArgumentParser(description='Qwenæ™ºèƒ½å­—å¹•åŠŸèƒ½æµ‹è¯•å’Œæ¨¡å‹ä¸‹è½½å·¥å…·')
    parser.add_argument('--download', action='store_true', help='ä¸‹è½½æ¨¡å‹è€Œä¸è¿è¡Œæµ‹è¯•')
    parser.add_argument('--model', choices=['realtime', 'refinement', 'all'], default='realtime',
                       help='æŒ‡å®šè¦ä¸‹è½½çš„æ¨¡å‹ç±»å‹ (realtime: å®æ—¶ç¿»è¯‘æ¨¡å‹, refinement: å­—å¹•ä¼˜åŒ–æ¨¡å‹, all: æ‰€æœ‰æ¨¡å‹)')
    parser.add_argument('--device', choices=['cpu', 'cuda'], default=None,
                       help='æŒ‡å®šè®¾å¤‡ç±»å‹ (cpu/cuda)ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹')
    parser.add_argument('--fp16', action='store_true', help='ä½¿ç”¨FP16ç²¾åº¦ï¼ˆä»…GPUæ”¯æŒï¼‰')
    parser.add_argument('--skip-tests', action='store_true', help='è·³è¿‡æµ‹è¯•ï¼Œä»…ä¸‹è½½æ¨¡å‹')
    
    args = parser.parse_args()
    
    print("\n" + "ğŸ§ª" * 30)
    print("   Qwenæ™ºèƒ½å­—å¹•æ ¡å¯¹åŠŸèƒ½æµ‹è¯•")
    print("ğŸ§ª" * 30 + "\n")
    
    # æ£€æµ‹è®¾å¤‡
    device = args.device
    if device is None:
        try:
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        except:
            device = 'cpu'
    
    # å¦‚æœæŒ‡å®šäº†ä¸‹è½½æ¨¡å¼
    if args.download or args.skip_tests:
        print(f"\n{'='*60}")
        print("ğŸ“¥ æ¨¡å‹ä¸‹è½½æ¨¡å¼")
        print(f"{'='*60}")
        print(f"ç›®æ ‡è®¾å¤‡: {device}")
        print(f"ä½¿ç”¨FP16: {args.fp16 and device == 'cuda'}")
        print(f"æ¨¡å‹ç±»å‹: {args.model}")
        
        success = download_all_models(
            device=device,
            use_fp16=args.fp16 and device == 'cuda',
            model_filter=args.model
        )
        
        if success:
            print("\nâœ… æ‰€æœ‰æ¨¡å‹ä¸‹è½½æˆåŠŸï¼")
            print("\nğŸ“¦ å·²ä¸‹è½½çš„æ¨¡å‹å¯åœ¨ä»¥ä¸‹ç›®å½•æ‰¾åˆ°:")
            print(f"   {os.path.expanduser('~/.cache/huggingface/hub')}")
            print("\nğŸ’¡ ç°åœ¨å¯ä»¥å¯åŠ¨æœåŠ¡:")
            print("   python app.py")
        else:
            print("\nâš ï¸ éƒ¨åˆ†æ¨¡å‹ä¸‹è½½å¤±è´¥")
            print("ğŸ’¡ å¯ä»¥å°è¯•è®¾ç½®é•œåƒ:")
            print("   export HF_ENDPOINT=https://hf-mirror.com")
            print("   python test_qwen.py --download --model realtime")
        
        return success
    
    # æ­£å¸¸æµ‹è¯•æ¨¡å¼
    results = []
    
    # 1. æµ‹è¯•åº“å¯¼å…¥
    results.append(("åº“å¯¼å…¥", test_qwen_import()))
    
    # 2. æµ‹è¯•GPUæ£€æµ‹
    gpu_ok, device = test_gpu_detection()
    results.append(("GPUæ£€æµ‹", gpu_ok))
    
    # 3. æµ‹è¯•æ¨¡å‹ä¿¡æ¯
    model_info_ok, models = test_qwen_model_info()
    results.append(("æ¨¡å‹ä¿¡æ¯", model_info_ok))
    
    # 4. æµ‹è¯•è§„åˆ™ä¼˜åŒ–
    results.append(("è§„åˆ™ä¼˜åŒ–", test_rule_based_refinement()))
    
    # 5. æµ‹è¯•æ¨¡å‹åŠ è½½ï¼ˆå¯é€‰ï¼‰
    results.append(("æ¨¡å‹åŠ è½½", test_qwen_model_loading(device)))
    
    # 6. æµ‹è¯•APIï¼ˆå¯é€‰ï¼‰
    results.append(("APIç«¯ç‚¹", test_api_endpoints()))
    
    # è¾“å‡ºæµ‹è¯•æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{name:15} {status}")
    
    print("-" * 60)
    print(f"æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼QwenåŠŸèƒ½å·²å°±ç»ª")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. é¢„ä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼‰:")
        print("   python test_qwen.py --download --model realtime  # ä¸‹è½½å®æ—¶ç¿»è¯‘æ¨¡å‹")
        print("   python test_qwen.py --download --model refinement  # ä¸‹è½½å­—å¹•ä¼˜åŒ–æ¨¡å‹")
        print("   python test_qwen.py --download --model all  # ä¸‹è½½æ‰€æœ‰æ¨¡å‹")
        print("\n2. å¯åŠ¨æœåŠ¡: python app.py")
        print("3. è®¿é—®å®æ—¶è½¬å½•: http://localhost:5001/realtime.html")
        print("4. å¯ç”¨å­—å¹•ä¼˜åŒ–å¹¶é€‰æ‹©Qwenæ¨¡å‹")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
        print("\nå¸¸è§é—®é¢˜:")
        print("- æ¨¡å‹ä¸‹è½½å¤±è´¥: è®¾ç½®é•œåƒ export HF_ENDPOINT=https://hf-mirror.com")
        print("- æ˜¾å­˜ä¸è¶³: ä½¿ç”¨æ›´å°çš„æ¨¡å‹(Qwen3-0.6B æˆ– Qwen3-1.7B)")
        print("- CPUæ¨¡å¼æ…¢: è€ƒè™‘å‡çº§ç¡¬ä»¶æˆ–ä½¿ç”¨æœ¬åœ°è§„åˆ™ä¼˜åŒ–")
        print("\nğŸ’¡ å¯ä»¥å…ˆä¸‹è½½æ¨¡å‹:")
        print("   python test_qwen.py --download --model realtime")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
