#!/usr/bin/env python3
"""æµ‹è¯•ä¼˜åŒ–åçš„ç¿»è¯‘é€Ÿåº¦"""

import sys
import time
import torch

# è®¾ç½®ç¯å¢ƒ
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

print("ğŸ”§ å¯¼å…¥æ¨¡å—...")
from transformers import AutoModelForCausalLM, AutoTokenizer

print("âœ… æ¨¡å—å¯¼å…¥å®Œæˆ\n")

# æµ‹è¯•å‚æ•°
MODEL_ID = "Qwen/Qwen3-0.6B"
TEST_TEXTS = [
    "Hello, how are you?",
    "I don't know how to do it.",
    "Yeah, I agree.",
]

print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {MODEL_ID}")
start_time = time.time()

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)
model.eval()

load_time = time.time() - start_time
print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’\n")

# æ–°çš„ç®€æ´prompt
SYSTEM_PROMPT = """ä½ æ˜¯ä¸“ä¸šå­—å¹•ç¿»è¯‘åŠ©æ‰‹ã€‚ç›´æ¥è¾“å‡ºç®€æ´çš„ä¸­æ–‡ç¿»è¯‘ã€‚

è§„åˆ™ï¼š
- åªè¾“å‡ºä¸­æ–‡ç¿»è¯‘
- ä¸è¦è§£é‡Šã€ä¸è¦æ€è€ƒè¿‡ç¨‹
- ä¿æŒè‡ªç„¶æµç•…
- ä¿ç•™è¯­æ°”å’Œæƒ…æ„Ÿ"""

# æµ‹è¯•ç¿»è¯‘
print("=" * 60)
print("å¼€å§‹ç¿»è¯‘æµ‹è¯•ï¼ˆä¼˜åŒ–promptï¼‰")
print("=" * 60)

for i, text in enumerate(TEST_TEXTS, 1):
    print(f"\nã€æµ‹è¯• {i}/{len(TEST_TEXTS)}ã€‘")
    print(f"åŸæ–‡: {text}")
    
    # æ„å»ºprompt
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": f"ç¿»è¯‘æˆä¸­æ–‡ï¼š\n{text}"}
    ]
    
    text_input = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    model_inputs = tokenizer([text_input], return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆç¿»è¯‘
    start = time.time()
    
    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=64,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    elapsed = time.time() - start
    
    # è§£ç 
    output_ids = [
        output_id[len(input_id):] 
        for input_id, output_id in zip(model_inputs.input_ids, generated_ids)
    ]
    
    translation = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
    
    # æ¸…ç† <think> æ ‡ç­¾
    if '<think>' in translation:
        parts = translation.split('</think>')
        if len(parts) > 1:
            translation = parts[1].strip()
    
    print(f"è¯‘æ–‡: {translation}")
    print(f"â±ï¸  è€—æ—¶: {elapsed:.3f}ç§’")

print("\n" + "=" * 60)
print("æµ‹è¯•å®Œæˆ")
print("=" * 60)
