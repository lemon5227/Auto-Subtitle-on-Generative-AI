#!/usr/bin/env python3
"""
é€šç”¨ GPU æ£€æµ‹å’Œé€‚é…æ¨¡å—
ğŸš€ æ™ºèƒ½æ£€æµ‹å¹¶é€‚é… NVIDIAã€AMDã€Apple Silicon å’Œ CPU
"""
import os
import sys
import subprocess
import logging
from typing import Tuple, Dict, Optional, List

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GPUDetector:
    """é€šç”¨GPUæ£€æµ‹å™¨ - è‡ªåŠ¨è¯†åˆ«æœ€ä½³å¯ç”¨è®¡ç®—è®¾å¤‡"""
    
    def __init__(self):
        self.gpu_info = self._detect_all_gpus()
        self.device_info = self._get_device_info()
        
    def _run_command(self, cmd: str) -> str:
        """å®‰å…¨æ‰§è¡Œç³»ç»Ÿå‘½ä»¤"""
        try:
            result = subprocess.run(
                cmd.split(), 
                capture_output=True, 
                text=True, 
                timeout=10
            )
            return result.stdout.strip() if result.returncode == 0 else ""
        except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
            return ""
    
    def _detect_nvidia_gpu(self) -> Dict:
        """æ£€æµ‹ NVIDIA GPU"""
        nvidia_info = {
            'available': False,
            'gpus': [],
            'driver_version': '',
            'cuda_version': '',
            'total_memory': 0
        }
        
        # æ£€æŸ¥ nvidia-smi
        nvidia_smi = self._run_command("nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits")
        if nvidia_smi:
            lines = nvidia_smi.split('\n')
            for line in lines:
                if line.strip():
                    parts = [p.strip() for p in line.split(',')]
                    if len(parts) >= 3:
                        gpu_name = parts[0]
                        memory_mb = int(parts[1]) if parts[1].isdigit() else 0
                        driver_ver = parts[2]
                        
                        nvidia_info['gpus'].append({
                            'name': gpu_name,
                            'memory_mb': memory_mb
                        })
                        nvidia_info['total_memory'] += memory_mb
                        nvidia_info['driver_version'] = driver_ver
                        nvidia_info['available'] = True
        
        # æ£€æŸ¥ CUDA ç‰ˆæœ¬
        nvcc_output = self._run_command("nvcc --version")
        if nvcc_output:
            for line in nvcc_output.split('\n'):
                if 'release' in line.lower():
                    nvidia_info['cuda_version'] = line.split('release')[-1].strip().split(',')[0]
                    break
        
        return nvidia_info
    
    def _detect_amd_gpu(self) -> Dict:
        """æ£€æµ‹ AMD GPU"""
        amd_info = {
            'available': False,
            'gpus': [],
            'rocm_version': '',
            'total_memory': 0
        }
        
        # é€šè¿‡ lspci æ£€æµ‹ AMD GPU
        lspci_output = self._run_command("lspci")
        amd_gpus = []
        for line in lspci_output.split('\n'):
            if 'amd' in line.lower() and ('vga' in line.lower() or 'display' in line.lower()):
                # æå– GPU åç§°
                gpu_name = line.split(': ')[-1] if ': ' in line else line
                amd_gpus.append(gpu_name.strip())
        
        if amd_gpus:
            amd_info['available'] = True
            for gpu_name in amd_gpus:
                amd_info['gpus'].append({
                    'name': gpu_name,
                    'memory_mb': 0  # AMD æ˜¾å­˜ä¿¡æ¯è¾ƒéš¾è·å–ï¼Œæš‚è®¾ä¸º0
                })
        
        # æ£€æŸ¥ ROCm
        if os.path.exists('/opt/rocm'):
            rocm_info = self._run_command("/opt/rocm/bin/rocm_agent_enumerator")
            if rocm_info or os.path.exists('/opt/rocm/bin/rocminfo'):
                amd_info['rocm_version'] = 'Installed'
        
        # æ£€æŸ¥å†…æ ¸æ¨¡å—
        lsmod_output = self._run_command("lsmod")
        if 'amdgpu' in lsmod_output:
            amd_info['driver_loaded'] = True
        
        return amd_info
    
    def _detect_apple_gpu(self) -> Dict:
        """æ£€æµ‹ Apple Silicon GPU"""
        apple_info = {
            'available': False,
            'chip': '',
            'memory': 0
        }
        
        # æ£€æŸ¥æ˜¯å¦ä¸º macOS
        if sys.platform != 'darwin':
            return apple_info
            
        # æ£€æŸ¥èŠ¯ç‰‡ä¿¡æ¯
        system_profiler = self._run_command("system_profiler SPHardwareDataType")
        for line in system_profiler.split('\n'):
            if 'Chip:' in line:
                apple_info['chip'] = line.split('Chip:')[-1].strip()
                if 'M1' in apple_info['chip'] or 'M2' in apple_info['chip'] or 'M3' in apple_info['chip']:
                    apple_info['available'] = True
            elif 'Memory:' in line:
                memory_str = line.split('Memory:')[-1].strip()
                if 'GB' in memory_str:
                    apple_info['memory'] = int(memory_str.split()[0])
        
        return apple_info
    
    def _detect_all_gpus(self) -> Dict:
        """æ£€æµ‹æ‰€æœ‰å¯ç”¨çš„GPU"""
        return {
            'nvidia': self._detect_nvidia_gpu(),
            'amd': self._detect_amd_gpu(),
            'apple': self._detect_apple_gpu()
        }
    
    def _test_pytorch_device(self, device: str) -> bool:
        """æµ‹è¯• PyTorch è®¾å¤‡å¯ç”¨æ€§"""
        try:
            import torch
            
            if device == 'cuda':
                if not torch.cuda.is_available():
                    return False
                # ç®€å•çš„ CUDA æµ‹è¯•
                try:
                    x = torch.randn(100, 100).cuda()
                    y = torch.matmul(x, x)
                    del x, y
                    torch.cuda.empty_cache()
                    return True
                except Exception as e:
                    logger.warning(f"CUDAæµ‹è¯•å¤±è´¥: {e}")
                    return False
                    
            elif device == 'mps':
                if not (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()):
                    return False
                # ç®€å•çš„ MPS æµ‹è¯•
                try:
                    x = torch.randn(100, 100).to('mps')
                    y = torch.matmul(x, x)
                    del x, y
                    return True
                except Exception as e:
                    logger.warning(f"MPSæµ‹è¯•å¤±è´¥: {e}")
                    return False
                    
            elif device == 'cpu':
                return True
                
            return False
            
        except ImportError:
            logger.warning("PyTorch æœªå®‰è£…ï¼Œæ— æ³•æµ‹è¯•è®¾å¤‡")
            return False
    
    def _get_device_info(self) -> Dict:
        """è·å–æ¨èçš„ PyTorch è®¾å¤‡ä¿¡æ¯"""
        device_info = {
            'recommended_device': 'cpu',
            'available_devices': ['cpu'],
            'gpu_type': 'none',
            'gpu_name': '',
            'memory_gb': 0,
            'performance_level': 'basic',
            'optimization_tips': []
        }
        
        # NVIDIA GPU (æœ€é«˜ä¼˜å…ˆçº§)
        if self.gpu_info['nvidia']['available']:
            if self._test_pytorch_device('cuda'):
                device_info['recommended_device'] = 'cuda'
                device_info['available_devices'].append('cuda')
                device_info['gpu_type'] = 'nvidia'
                
                # è·å–æœ€ä½³ GPU ä¿¡æ¯
                gpus = self.gpu_info['nvidia']['gpus']
                if gpus:
                    best_gpu = max(gpus, key=lambda g: g['memory_mb'])
                    device_info['gpu_name'] = best_gpu['name']
                    device_info['memory_gb'] = best_gpu['memory_mb'] // 1024
                    
                    # æ€§èƒ½ç­‰çº§åˆ¤æ–­
                    if device_info['memory_gb'] >= 12:
                        device_info['performance_level'] = 'excellent'
                    elif device_info['memory_gb'] >= 8:
                        device_info['performance_level'] = 'good'
                    elif device_info['memory_gb'] >= 4:
                        device_info['performance_level'] = 'acceptable'
                    else:
                        device_info['performance_level'] = 'limited'
                
                device_info['optimization_tips'] = [
                    "ä½¿ç”¨ CUDA åŠ é€Ÿè·å¾—æœ€ä½³æ€§èƒ½",
                    "å¤§æ¨¡å‹éœ€è¦è¶³å¤Ÿæ˜¾å­˜ï¼Œå»ºè®®é€‰æ‹©åˆé€‚çš„æ¨¡å‹å¤§å°",
                    "å¯è°ƒæ•´æ‰¹å¤„ç†å¤§å°ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨"
                ]
        
        # AMD GPU (ç¬¬äºŒä¼˜å…ˆçº§ï¼ŒROCm æ”¯æŒ)
        elif self.gpu_info['amd']['available']:
            # æµ‹è¯•ROCmæ”¯æŒ
            rocm_available = self._test_pytorch_device('cuda') and self.gpu_info['amd'].get('rocm_version')
            
            if rocm_available:
                device_info['recommended_device'] = 'cuda'  # ROCm é€šè¿‡ CUDA API
                device_info['available_devices'].append('cuda')
                device_info['gpu_type'] = 'amd'
                
                gpus = self.gpu_info['amd']['gpus']
                if gpus:
                    device_info['gpu_name'] = gpus[0]['name']
                    
                    # AMD GPU æ€§èƒ½é¢„ä¼° (åŸºäºå‹å·)
                    gpu_name = gpus[0]['name'].lower()
                    if any(x in gpu_name for x in ['rx 7900', 'rx 7800', 'rx 6900', 'rx 6800']):
                        device_info['performance_level'] = 'excellent'
                        device_info['memory_gb'] = 12  # ä¼°ç®—
                    elif any(x in gpu_name for x in ['rx 7700', 'rx 7600', 'rx 6700', 'rx 6600']):
                        device_info['performance_level'] = 'good'
                        device_info['memory_gb'] = 8   # ä¼°ç®—
                    elif any(x in gpu_name for x in ['rx 5700', 'rx 5600', 'rx 5500']):
                        device_info['performance_level'] = 'acceptable'
                        device_info['memory_gb'] = 6   # ä¼°ç®—
                    else:
                        device_info['performance_level'] = 'limited'
                        device_info['memory_gb'] = 4   # ä¼°ç®—
                
                device_info['optimization_tips'] = [
                    "ä½¿ç”¨ ROCm è·å¾— AMD GPU åŠ é€Ÿ",
                    "ç¡®ä¿å®‰è£…äº†æ­£ç¡®çš„ ROCm é©±åŠ¨",
                    "éƒ¨åˆ†åŠŸèƒ½å¯èƒ½éœ€è¦ç‰¹å®šçš„ç¯å¢ƒå˜é‡",
                    "å¦‚é‡é—®é¢˜å¯å›é€€åˆ° CPU æ¨¡å¼"
                ]
            else:
                # AMD GPUå­˜åœ¨ä½†ROCmä¸å¯ç”¨ï¼Œè®°å½•ä¿¡æ¯ä½†å›é€€åˆ°CPU
                gpus = self.gpu_info['amd']['gpus']
                if gpus:
                    device_info['gpu_name'] = f"{gpus[0]['name']} (ROCmä¸å¯ç”¨)"
                
                device_info['optimization_tips'] = [
                    "æ£€æµ‹åˆ°AMD GPUä½†ROCmä¸å¯ç”¨",
                    "å®‰è£…ROCmé©±åŠ¨: sudo apt install rocm-dev",
                    "å®‰è£…PyTorch ROCmç‰ˆæœ¬è·å¾—GPUåŠ é€Ÿ",
                    "å½“å‰ä½¿ç”¨CPUæ¨¡å¼ç¡®ä¿ç¨³å®šè¿è¡Œ"
                ]
        
        # Apple Silicon GPU (ç¬¬ä¸‰ä¼˜å…ˆçº§)
        elif self.gpu_info['apple']['available']:
            if self._test_pytorch_device('mps'):
                device_info['recommended_device'] = 'mps'
                device_info['available_devices'].append('mps')
                device_info['gpu_type'] = 'apple'
                device_info['gpu_name'] = self.gpu_info['apple']['chip']
                device_info['memory_gb'] = self.gpu_info['apple']['memory']
                device_info['performance_level'] = 'good'
                
                device_info['optimization_tips'] = [
                    "ä½¿ç”¨ Apple Metal Performance Shaders åŠ é€Ÿ",
                    "åœ¨ Apple Silicon Mac ä¸Šè·å¾—è‰¯å¥½æ€§èƒ½",
                    "ç»Ÿä¸€å†…å­˜æ¶æ„ï¼Œæ˜¾å­˜å³ç³»ç»Ÿå†…å­˜"
                ]
        
        # CPU æ¨¡å¼ (æœ€åé€‰æ‹©)
        else:
            device_info['optimization_tips'] = [
                "å½“å‰ä½¿ç”¨ CPU æ¨¡å¼",
                "æ¨èé€‰æ‹©è¾ƒå°çš„æ¨¡å‹ä»¥è·å¾—æ›´å¥½å“åº”é€Ÿåº¦",
                "è€ƒè™‘ä½¿ç”¨é‡åŒ–æ¨¡å‹å‡å°‘è®¡ç®—é‡",
                "å¤šçº¿ç¨‹è®¾ç½®: export OMP_NUM_THREADS=4"
            ]
        
        return device_info
    
    def get_recommended_device(self) -> str:
        """è·å–æ¨èçš„è®¡ç®—è®¾å¤‡"""
        return self.device_info['recommended_device']
    
    def get_device_summary(self) -> str:
        """è·å–è®¾å¤‡ä¿¡æ¯æ‘˜è¦"""
        info = self.device_info
        gpu_type_map = {
            'nvidia': 'ğŸŸ¢ NVIDIA GPU',
            'amd': 'ğŸ”´ AMD GPU', 
            'apple': 'ğŸŸ¡ Apple Silicon',
            'none': 'ğŸ”µ CPU'
        }
        
        summary = f"{gpu_type_map.get(info['gpu_type'], 'ğŸ”µ CPU')}"
        
        if info['gpu_name']:
            summary += f" ({info['gpu_name']}"
            if info['memory_gb'] > 0:
                summary += f", {info['memory_gb']}GB"
            summary += ")"
        
        summary += f" - {info['performance_level']} æ€§èƒ½"
        
        return summary
    
    def get_optimization_tips(self) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        return self.device_info['optimization_tips']
    
    def print_detection_report(self):
        """æ‰“å°å®Œæ•´çš„æ£€æµ‹æŠ¥å‘Š"""
        print("ğŸš€ GPU æ£€æµ‹å’Œè®¾å¤‡é€‰æ‹©æŠ¥å‘Š")
        print("=" * 50)
        
        # NVIDIA ä¿¡æ¯
        nvidia = self.gpu_info['nvidia']
        print(f"ğŸŸ¢ NVIDIA GPU: {'âœ… æ£€æµ‹åˆ°' if nvidia['available'] else 'âŒ æœªæ£€æµ‹åˆ°'}")
        if nvidia['available']:
            for i, gpu in enumerate(nvidia['gpus']):
                print(f"   GPU {i+1}: {gpu['name']} ({gpu['memory_mb']}MB)")
            if nvidia['driver_version']:
                print(f"   é©±åŠ¨ç‰ˆæœ¬: {nvidia['driver_version']}")
            if nvidia['cuda_version']:
                print(f"   CUDA ç‰ˆæœ¬: {nvidia['cuda_version']}")
        
        # AMD ä¿¡æ¯  
        amd = self.gpu_info['amd']
        print(f"ğŸ”´ AMD GPU: {'âœ… æ£€æµ‹åˆ°' if amd['available'] else 'âŒ æœªæ£€æµ‹åˆ°'}")
        if amd['available']:
            for i, gpu in enumerate(amd['gpus']):
                print(f"   GPU {i+1}: {gpu['name']}")
            if amd.get('rocm_version'):
                print(f"   ROCm: {amd['rocm_version']}")
        
        # Apple ä¿¡æ¯
        apple = self.gpu_info['apple']
        print(f"ğŸŸ¡ Apple GPU: {'âœ… æ£€æµ‹åˆ°' if apple['available'] else 'âŒ æœªæ£€æµ‹åˆ°'}")
        if apple['available']:
            print(f"   èŠ¯ç‰‡: {apple['chip']}")
            if apple['memory'] > 0:
                print(f"   å†…å­˜: {apple['memory']}GB")
        
        print(f"\nğŸ¯ æ¨èè®¾å¤‡: {self.get_recommended_device()}")
        print(f"ğŸ“Š è®¾å¤‡æ‘˜è¦: {self.get_device_summary()}")
        
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for tip in self.get_optimization_tips():
            print(f"   â€¢ {tip}")


def get_optimal_device() -> Tuple[str, Dict]:
    """
    è·å–æœ€ä½³è®¡ç®—è®¾å¤‡
    è¿”å›: (device_name, device_info)
    """
    detector = GPUDetector()
    return detector.get_recommended_device(), detector.device_info


def create_device_environment() -> Dict[str, str]:
    """åˆ›å»ºé’ˆå¯¹æ£€æµ‹åˆ°çš„è®¾å¤‡çš„ç¯å¢ƒå˜é‡"""
    detector = GPUDetector()
    device_info = detector.device_info
    env_vars = {}
    
    if device_info['gpu_type'] == 'nvidia':
        # NVIDIA CUDA ä¼˜åŒ–
        env_vars.update({
            'CUDA_VISIBLE_DEVICES': '0',
            'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:128',
        })
    elif device_info['gpu_type'] == 'amd':
        # AMD ROCm ä¼˜åŒ–
        env_vars.update({
            'HSA_OVERRIDE_GFX_VERSION': '10.3.0',
            'ROCM_PATH': '/opt/rocm',
            'HIP_VISIBLE_DEVICES': '0',
            'PYTORCH_HIP_ALLOC_CONF': 'max_split_size_mb:128',
        })
    elif device_info['gpu_type'] == 'apple':
        # Apple MPS ä¼˜åŒ–
        env_vars.update({
            'PYTORCH_MPS_HIGH_WATERMARK_RATIO': '0.0',
        })
    else:
        # CPU ä¼˜åŒ–
        env_vars.update({
            'OMP_NUM_THREADS': '4',
            'MKL_NUM_THREADS': '4',
        })
    
    # é€šç”¨ä¼˜åŒ–
    env_vars.update({
        'TOKENIZERS_PARALLELISM': 'false',
        'PYTHONUNBUFFERED': '1',
    })
    
    return env_vars


if __name__ == "__main__":
    # æµ‹è¯•è„šæœ¬
    detector = GPUDetector()
    detector.print_detection_report()
    
    print(f"\nğŸ”§ æ¨èç¯å¢ƒå˜é‡:")
    env_vars = create_device_environment()
    for key, value in env_vars.items():
        print(f"   export {key}={value}")